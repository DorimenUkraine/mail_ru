{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В изысканиях опирался на данную статью: https://medium.com/radon-dev/als-implicit-collaborative-filtering-5ed653ba39fe\n",
    "Предварительно прослушал про ALS https://www.youtube.com/watch?v=NlNLtPqlCK0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.sparse as sparse\n",
    "from scipy.sparse.linalg import spsolve\n",
    "import random\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import implicit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('input/train_2.csv')\n",
    "test = pd.read_csv('input/test_2.csv')\n",
    "user_features = pd.read_csv('input/user-features_2.csv')\n",
    "item_features = pd.read_csv('input/item-features_2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_full = train.merge(user_features, how='left', left_on='user_id', right_on='user_id')\n",
    "train_full = train_full.merge(item_features, how='left', left_on='item_id', right_on='item_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to datetime trains\n",
    "\n",
    "train_full['created'] = pd.to_datetime(train_full['timestamp'],unit='s')\n",
    "\n",
    "# create new date features\n",
    "\n",
    "train_full['created_month'] = train_full['created'].dt.month\n",
    "train_full['created_day'] = train_full['created'].dt.day\n",
    "train_full['created_dayofweek'] = train_full['created'].dt.dayofweek\n",
    "train_full['created_hour'] = train_full['created'].dt.round('H').dt.hour\n",
    "\n",
    "def f(x):\n",
    "    if (x > 4) and (x <= 8):\n",
    "        return 'Early Morning'\n",
    "    elif (x > 8) and (x <= 12 ):\n",
    "        return 'Morning'\n",
    "    elif (x > 12) and (x <= 16):\n",
    "        return'Noon'\n",
    "    elif (x > 16) and (x <= 20) :\n",
    "        return 'Eve'\n",
    "    elif (x > 20) and (x <= 24):\n",
    "        return'Night'\n",
    "    elif (x <= 4):\n",
    "        return'Late Night'\n",
    "    \n",
    "    \n",
    "train_full['part_of_day'] = train_full['created_hour'].apply(f)\n",
    "\n",
    "# train_full = train_full.drop(['created', 'timestamp'], axis=1)\n",
    "\n",
    "# для One-Hot Encoding в pandas есть готовая функция - get_dummies. Особенно радует параметр dummy_na\n",
    "train_full = pd.get_dummies(train_full, columns=[ 'part_of_day',], dummy_na=True)\n",
    "\n",
    "# удалю окончательно колонку даты и времени нажатия на баннер\n",
    "train_full = train_full.drop(['created'], axis = 1)\n",
    "\n",
    "if train_full.part_of_day_nan.isnull().sum() == 0:\n",
    "    train_full = train_full.drop(['part_of_day_nan'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to datetime test\n",
    "\n",
    "test['created'] = pd.to_datetime(test['timestamp'],unit='s')\n",
    "\n",
    "# create new date features\n",
    "\n",
    "test['created_month'] = test['created'].dt.month\n",
    "test['created_day'] = test['created'].dt.day\n",
    "test['created_dayofweek'] = test['created'].dt.dayofweek\n",
    "test['created_hour'] = test['created'].dt.round('H').dt.hour\n",
    "\n",
    "def f(x):\n",
    "    if (x > 4) and (x <= 8):\n",
    "        return 'Early Morning'\n",
    "    elif (x > 8) and (x <= 12 ):\n",
    "        return 'Morning'\n",
    "    elif (x > 12) and (x <= 16):\n",
    "        return'Noon'\n",
    "    elif (x > 16) and (x <= 20) :\n",
    "        return 'Eve'\n",
    "    elif (x > 20) and (x <= 24):\n",
    "        return'Night'\n",
    "    elif (x <= 4):\n",
    "        return'Late Night'\n",
    "    \n",
    "    \n",
    "test['part_of_day'] = test['created_hour'].apply(f)\n",
    "\n",
    "# train_full = train_full.drop(['created', 'timestamp'], axis=1)\n",
    "\n",
    "# для One-Hot Encoding в pandas есть готовая функция - get_dummies. Особенно радует параметр dummy_na\n",
    "test = pd.get_dummies(test, columns=[ 'part_of_day',], dummy_na=True)\n",
    "\n",
    "# удалю окончательно колонку даты и времени нажатия на баннер\n",
    "test = test.drop(['created'], axis = 1)\n",
    "\n",
    "if test.part_of_day_nan.isnull().sum() == 0:\n",
    "    test = test.drop(['part_of_day_nan'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(497, 444)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "banner_pivot=train_full.pivot(index='user_id',columns='item_id',values='like')\n",
    "banner_pivot=banner_pivot.fillna(0)\n",
    "banner_pivot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 497 entries, 0 to 496\n",
      "Columns: 444 entries, 0 to 443\n",
      "dtypes: float64(444)\n",
      "memory usage: 1.7 MB\n"
     ]
    }
   ],
   "source": [
    "sparse_matrix = train_full.pivot(index='user_id',columns='item_id',values='like')\n",
    "sparse_matrix = sparse_matrix.fillna(0)\n",
    "sparse_matrix.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'csr_matrix' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-ac9a28b90d3b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msparse_matrix_sparse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcsr_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msparse_matrix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0msparse_matrix_sparse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'csr_matrix' is not defined"
     ]
    }
   ],
   "source": [
    "# sparse_matrix_sparse = csr_matrix(sparse_matrix.values)\n",
    "# sparse_matrix_sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# The implicit library expects data as a item-user matrix so we\n",
    "# create two matricies, one for fitting the model (item-user) \n",
    "# and one for recommendations (user-item)\n",
    "sparse_item_user = sparse.csr_matrix((train_full['like'].astype(float), (train_full['item_id'], train_full['user_id'])))\n",
    "sparse_user_item = sparse.csr_matrix((train_full['like'].astype(float), (train_full['user_id'], train_full['item_id'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<497x444 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 8674 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparse_user_item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "784826216cee4d6594bc5d3ebe93eb27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=40.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize the als model and fit it using the sparse item-user matrix\n",
    "model = implicit.als.AlternatingLeastSquares(factors=1, regularization=0.01, iterations=40)\n",
    "\n",
    "# Calculate the confidence by multiplying it by our alpha value.\n",
    "alpha_val = 15\n",
    "data_conf = (sparse_item_user * alpha_val).astype('double')\n",
    "\n",
    "#Fit the model\n",
    "model.fit(data_conf)  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find Similar Items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400\n",
      "304\n",
      "382\n",
      "292\n",
      "287\n",
      "308\n",
      "290\n",
      "286\n",
      "396\n",
      "330\n",
      "406\n",
      "393\n",
      "440\n",
      "428\n",
      "441\n",
      "379\n",
      "288\n",
      "327\n",
      "244\n",
      "340\n"
     ]
    }
   ],
   "source": [
    "#---------------------\n",
    "# FIND SIMILAR ITEMS\n",
    "#---------------------\n",
    "\n",
    "# Find the 10 most similar\n",
    "item_id = 400\n",
    "n_similar = 20\n",
    "\n",
    "# Use implicit to get similar items.\n",
    "similar = model.similar_items(item_id, n_similar)\n",
    "\n",
    "# Print the names of our most similar artists\n",
    "for item in similar:\n",
    "    idx, score = item\n",
    "    print(train_full.item_id.loc[train_full.item_id == idx].iloc[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0   1   2   3   4   5   6   7   8   9   10   11   12  13  14   15   16  \\\n",
      "166  76  22  35  72  40  80  37  58  65  66   11   32   21   5  60   87  146   \n",
      "26   76  22  35  72  40  80  37  58  65  66   11   67   32  21   5   60   87   \n",
      "41   76  22  35  72  40  80  37  58  65  66   11   67   32  21   5   60   87   \n",
      "286  76  22  35  72  80  37  58  65  66  11   67   32   21   5  60   87  146   \n",
      "108  76  22  35  72  40  80  37  58  66  11   67   32   21   5  60   87  146   \n",
      "..   ..  ..  ..  ..  ..  ..  ..  ..  ..  ..  ...  ...  ...  ..  ..  ...  ...   \n",
      "190  76  35  37  65  66  32  21   5  60  87  146  119   33  36  70   30  147   \n",
      "181  76  22  35  72  40  80  37  58  65  66   11   67   32  21   5   60  146   \n",
      "448  76  22  35  72  37  58  65  66  67  32   21   60   87   7  33   17   36   \n",
      "124  76  22  35  72  40  80  58  65  66  11   67   32   21   5  87  146    7   \n",
      "167  22  40  80  37  58  66  11  21   5  60   87  146  119  78  33  172  147   \n",
      "\n",
      "      17   18   19  \n",
      "166    7   44  119  \n",
      "26   146    7   44  \n",
      "41   146   44  119  \n",
      "286    7   44  119  \n",
      "108    7   44  119  \n",
      "..   ...  ...  ...  \n",
      "190   59   19   71  \n",
      "181    7   44  119  \n",
      "448   70  155   30  \n",
      "124   44  119   78  \n",
      "167   88   59   19  \n",
      "\n",
      "[497 rows x 20 columns]\n"
     ]
    }
   ],
   "source": [
    "#------------------------------\n",
    "# CREATE USER RECOMMENDATIONS\n",
    "#------------------------------\n",
    "\n",
    "recommendations = {}\n",
    "\n",
    "# Create recommendations for user\n",
    "for u in test['user_id']:\n",
    "# for u in range(2):\n",
    "    \n",
    "    user_id = u\n",
    "\n",
    "    # Use the implicit recommender.\n",
    "    recommended = model.recommend(user_id, sparse_user_item, 20)\n",
    "\n",
    "    item_id = []\n",
    "    scores = []\n",
    "\n",
    "    # Get artist names from ids\n",
    "    for item in recommended:\n",
    "        idx, score = item\n",
    "        item_id.append(train_full.item_id.loc[train_full.item_id == idx].iloc[0])\n",
    "        scores.append(score)\n",
    "\n",
    "    # Create a dataframe of artist names and scores\n",
    "    # recommendations = pd.DataFrame({'user_id': user_id, 'item_id': item_id, 'score': scores})\n",
    "    recommendations.update({user_id: item_id})\n",
    "\n",
    "\n",
    "rec_df = pd.DataFrame.from_dict(recommendations)\n",
    "    \n",
    "print(rec_df.T)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Экспорт предсказаний в csv\n",
    "pd.DataFrame(rec_df.T).to_csv('input/out.csv', index=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
